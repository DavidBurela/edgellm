{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative AI at the Edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This notebook goes through the steps required to chunk, process, generate embeddings, and answer user prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">1. Import relevant libraries</font>\n",
    "\n",
    "<font size=\"2\">We use langchain as our main orchestrator</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from werkzeug.utils import secure_filename\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.vectorstores import Chroma\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">2. Import relevant Large Langauge Models</font>\n",
    "\n",
    "<font size=\"2\">We use Llama v2 that has been quantized (shrunk smartly). Llama 2 is a family of generative text models that are optimized for assistant-like chat use cases or can be adapted for a variety of natural language generation tasks\n",
    "\n",
    "This can take a few minutes to run as the model is around 3GB in size\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating model...\")\n",
    "\n",
    "model = LlamaCpp(model_path=\"./models/llama-2-7b.Q4_K_M.gguf\", n_threads=4)\n",
    "embeddings = LlamaCppEmbeddings(model_path=\"./models/llama-2-7b.Q4_K_M.gguf\", n_threads=4)\n",
    "\n",
    "print(\"Model created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">3. Load in texts</font>\n",
    "\n",
    "<font size=\"2\">In order to fine tune our model we want to create to embeddings which take in a more specific piece of text and creates word embeddings which can be used\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"./data/codereviewer-short.txtx\"\n",
    "\n",
    "def load_file(path):\n",
    "    # if the file extension is .txt, load as text\n",
    "    if path.endswith(\".txt\"):\n",
    "        loader = TextLoader(path)\n",
    "        documents = loader.load()\n",
    "    # if the file extension is .pdf, load as pdf\n",
    "    elif path.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(path)\n",
    "        documents = loader.load_and_split()\n",
    "\n",
    "    return documents\n",
    "\n",
    "documents = load_file(currentDocumentPath)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">4. Chunk texts</font>\n",
    "\n",
    "<font size=\"2\">\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "print(\"text was split\")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Found {len(texts)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">5. Generate Embeddings</font>\n",
    "\n",
    "<font size=\"2\">\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docSearch = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "qaChain = RetrievalQA.from_chain_type(llm=model, chain_type=\"stuff\", retriever=docSearch.as_retriever())\n",
    "print(qaChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">6. Run the prompt on the model</font>\n",
    "\n",
    "<font size=\"2\">\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the goal of a code review?\"\n",
    "response = model(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">7. Run the prompt on the model with embeddings</font>\n",
    "\n",
    "<font size=\"2\">\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the goal of a code review?\"\n",
    "response = qaChain.run(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">8. Explore which docs were relevant from the embeddings</font>\n",
    "\n",
    "<font size=\"2\">\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant embeddings (Could also use a vector database here)\n",
    "retriever = docSearch.as_retriever()\n",
    "docs = retriever.get_relevant_documents(prompt)\n",
    "for doc in docs:\n",
    "    print(\"###\")\n",
    "    print(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
